"""
Pipeline complet d'ingestion pour PDFs en RAM (aucune lecture disque).

√âtapes :
    1. Extraction du texte page par page depuis RAM
    2. Split en chunks (Documents LangChain)
    3. Insertion batch√©e dans le VectorStore (ex: Pinecone)
"""

#from __future__ import annotations

from typing import Iterable, List, Tuple
from langchain.schema import Document

from energydoc_talk_ai.core.logger import setup_logger
from energydoc_talk_ai.ingestion.vector_store import get_vector_store, add_documents_batch_safe
from energydoc_talk_ai.ingestion.extract_text import extract_text_from_pdf_ram
from energydoc_talk_ai.ingestion.split_text import split_pages_into_documents


# Logger
logger = setup_logger(logger_name="ingestion_pipeline_ram")


def ingestion_pipeline_ram(pdf_files: Iterable[Tuple[str, bytes]]) -> None:
    """
    Ingestion de PDFs charg√©s en m√©moire (RAM).

    Parameters
    ----------
    pdf_files : Iterable[Tuple[str, bytes]]
        Liste de tuples :
            (file_name, file_bytes)
        Exemple avec Streamlit :
            uploaded_files = st.file_uploader(..., accept_multiple_files=True)
            files = [(f.name, f.read()) for f in uploaded_files]

    Pipeline :
        1. Extraction du texte en RAM
        2. Split en chunks
        3. Indexation dans Pinecone
    """

    pdf_files = list(pdf_files)
    logger.info(f"Pipeline RAM : {len(pdf_files)} PDFs charg√©s.")

    all_documents: List[Document] = []

    # ======================================================================
    # üîÅ Boucle sur les PDFs
    # ======================================================================
    for file_name, file_bytes in pdf_files:

        logger.info(f"Traitement du PDF (RAM) : {file_name}")

        # 1) Extraction RAM
        logger.debug("Extraction du texte en RAM‚Ä¶")
        try:
            pages = extract_text_from_pdf_ram(file_bytes)
        except Exception as exc:
            logger.error(f"Erreur extraction {file_name} : {exc}")
            continue

        if not pages:
            logger.warning(f"Aucun texte extrait ‚Üí ignor√© : {file_name}")
            continue

        logger.info(f"{len(pages)} pages extraites depuis {file_name}.")

        # 2) Split pages ‚Üí chunks LangChain
        logger.debug("D√©coupage en chunks‚Ä¶")

        docs = split_pages_into_documents(
            pages=pages,
            source=file_name,  # metadata pour RAG
        )

        logger.info(f"{len(docs)} chunks g√©n√©r√©s pour {file_name}.")
        all_documents.extend(docs)

    # ======================================================================
    # üìå Indexation Pinecone
    # ======================================================================
    if not all_documents:
        logger.info("Aucun chunk √† indexer. Pipeline termin√©.")
        return

    logger.info(
        f"Indexation dans Pinecone : {len(all_documents)} chunks √† ins√©rer..."
    )

    vector_store = get_vector_store()

    # Google GenAI / Pinecone = batch obligatoire
    add_documents_batch_safe(vector_store, all_documents, batch_size=32)

    logger.info(
        f"Ingestion RAM termin√©e. "
        f"{len(all_documents)} chunks index√©s dans Pinecone."
    )
